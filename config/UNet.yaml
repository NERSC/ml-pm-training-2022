base: &base

  # Training config
  weight_init: {conv_init: 'normal', conv_scale: 0.02, conv_bias: 0.}
  lambda_rho: 1E-2 # weight for additional rho loss term
  full_scale: True # whether or not to use all 6 of the scales in U-Net
  batch_size: 64 # number of samples per training batch (global batch size for distributed training)
  base_batch_size: 64 # single GPU batch size
  num_epochs: 2
  enable_amp: False
  enable_apex: False
  enable_benchy: False
  enable_jit: False
  expdir: '/logs'
  # params for setting learning rate (cosine decay schedule):
  #   start_lr: initial learning rate
  #   end_lr:  final learning rate
  #   warmup_steps: number of steps over which to do linear warm-up of learning rate
  #                 *not used when training single-GPU or when scaling='none'*
  #   scaling: 'none' initial lr doesn't change with respect to number of GPUs (use for strong scaling)
  #            'linear' scale up according to lr_nGPU = nGPUs*lr_1GPU
  #            'sqrt' scale up according to lr_nGPU = sqrt(nGPUs)*lr_1GPU
  lr_schedule: {scaling: 'none', start_lr: 1.E-4, end_lr: 0., warmup_steps: 0}

  # Data
  data_loader_config: 'synthetic' # choices: 'synthetic', 'inmem', 'lowmem', 'dali-lowmem'
  box_size: [1024, 512] # total size of simulation boxes (train, validation)
  data_size: 64 # size of crops for training
  num_data_workers: 2 # number of dataloader worker threads per proc
  N_out_channels: 5
  # HDF5 files for PyTorch native dataloader
  train_path: '/data/downsamp_2048crop_train.h5'
  val_path: '/data/downsamp_1024crop_valid.h5'
  # numpy files for DALI dataloader
  train_path_npy_data: '/data/downsamp_2048crop_train_data.npy'
  train_path_npy_label: '/data/downsamp_2048crop_train_label.npy'
  val_path_npy_data: '/data/downsamp_1024crop_valid_data.npy'
  val_path_npy_label: '/data/downsamp_1024crop_valid_label.npy'
  use_cache: None # set this to a cache dir (e.g., NVMe on CoriGPU) if you copied data there


#------------------------------------------------------------------------

# A100 configs: for Perlmutter (crop sizes 64 and 96)

#------------------------------------------------------------------------

#                       ----   CROP SIZE 64   ----


A100_crop64_sqrt: &crop64_A100
  <<: *base
  data_loader_config: 'lowmem'
  data_size: 64
  Nsamples: 4096
  Nsamples_val: 512
  num_epochs: 80
  batch_size: 64
  lr_schedule: {scaling: 'sqrt', start_lr: 2.E-4, end_lr: 0., warmup_steps: 128}
  lambda_rho: 0.

bs64: &bs64
  <<: *base
  data_loader_config: 'dali-lowmem'
  data_size: 64
  Nsamples: 4096
  Nsamples_val: 512
  num_epochs: 80
  batch_size: 64
  lr_schedule: {scaling: 'sqrt', start_lr: 2.E-4, end_lr: 0., warmup_steps: 128}
  lambda_rho: 0.

bs256: # 4 GPUs
  <<: *bs64
  batch_size: 256

bs256_2x:
  <<: *bs64
  batch_size: 256
  num_epochs: 160

bs512_4x: # 8 GPUs
  <<: *bs64
  batch_size: 512
  num_epochs: 320

bs2048_16x: # 32 GPUs
  <<: *bs64
  batch_size: 2048
  Nsamples_val: 2048
  num_epochs: 1280

bs8192_64x: # 128 GPUs
  <<: *bs64
  batch_size: 8192
  Nsamples_val: 8192
  num_epochs: 5120
