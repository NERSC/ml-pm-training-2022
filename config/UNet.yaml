base: &base

  # Training config
  weight_init: {conv_init: 'normal', conv_scale: 0.02, conv_bias: 0.}
  lr_schedule: {start_lr: 1.E-4, end_lr: 0., warmup_steps: 0}
  lambda_rho: 1E-2 # weight for additional rho loss term
  full_scale: True # whether or not to use all 6 of the scales in U-Net
  batch_size: 1
  num_epochs: 2
  enable_amp: False
  enable_apex: False
  enable_benchy: False
  enable_jit: False
  ngpu: 1
  expdir: '/logs'

  # Data
  data_loader_config: 'synthetic' # choices: 'synthetic', 'inmem', 'lowmem', 'dali-lowmem'
  box_size: [1024, 512] # total size of simulation boxes (train, validation)
  data_size: 256 # size of crops for training
  num_data_workers: 2 # number of dataloader worker threads per proc
  N_out_channels: 5
  # HDF5 files for PyTorch native dataloader
  train_path: '/data/downsamp_2048crop_train.h5'
  val_path: '/data/downsamp_1024crop_valid.h5'
  # numpy files for DALI dataloader
  train_path_npy_data: '/data/downsamp_2048crop_train_data.npy'
  train_path_npy_label: '/data/downsamp_2048crop_train_label.npy'
  val_path_npy_data: '/data/downsamp_1024crop_valid_data.npy'
  val_path_npy_label: '/data/downsamp_1024crop_valid_label.npy'
  use_cache: None # set this to a cache dir (e.g., NVMe on CoriGPU) if you copied data there


#------------------------------------------------------------------------

# A100 configs: for Perlmutter (crop sizes 64 and 96)
# Learning rates are pre-computed to obey the square-root scaling rule lr_Ngpu = sqrt(Ngpu)*lr_singlegpu

#------------------------------------------------------------------------

#                       ----   CROP SIZE 64   ----


A100_crop64_1GPU: &crop64_A100
  <<: *base
  data_loader_config: 'lowmem'
  data_size: 64
  Nsamples: 4096
  Nsamples_val: 512
  num_epochs: 80
  batch_size: 64
  lr_schedule: {start_lr: 2.E-4, end_lr: 0., warmup_steps: 0}
  lambda_rho: 0.

A100_crop64_4GPU:
  <<: *crop64_A100
  lr_schedule: {start_lr: 4.E-4, end_lr: 0., warmup_steps: 128}
  ngpu: 4
  lambda_rho: 0.

A100_crop64_8GPU:
  <<: *crop64_A100
  lr_schedule: {start_lr: 5.66E-4, end_lr: 0., warmup_steps: 128}
  ngpu: 8
  lambda_rho: 0.

A100_crop64_32GPU:
  <<: *crop64_A100
  lr_schedule: {start_lr: 1.13E-3, end_lr: 0., warmup_steps: 128}
  ngpu: 32
  lambda_rho: 0.

A100_crop64_128GPU:
  <<: *crop64_A100
  lr_schedule: {start_lr: 2.26E-3, end_lr: 0., warmup_steps: 128}
  ngpu: 128
  lambda_rho: 0.



#                       ----   CROP SIZE 96   ----


A100_crop96_1GPU: &crop96_A100
  <<: *base
  data_loader_config: 'lowmem'
  data_size: 96
  Nsamples: 4096
  Nsamples_val: 256
  num_epochs: 80
  full_scale: False
  batch_size: 64
  lr_schedule: {start_lr: 2.E-4, end_lr: 0., warmup_steps: 0}
  lambda_rho: 0.


A100_crop96_4GPU:
  <<: *crop96_A100
  lr_schedule: {start_lr: 4.E-4, end_lr: 0., warmup_steps: 128}
  ngpu: 4
  lambda_rho: 0.

A100_crop96_8GPU:
  <<: *crop96_A100
  lr_schedule: {start_lr: 5.66E-4, end_lr: 0., warmup_steps: 128}
  ngpu: 8
  lambda_rho: 0.

A100_crop96_32GPU:
  <<: *crop96_A100
  lr_schedule: {start_lr: 1.13E-3, end_lr: 0., warmup_steps: 128}
  ngpu: 32
  lambda_rho: 0.

A100_crop96_128GPU:
  <<: *crop96_A100
  lr_schedule: {start_lr: 2.26E-3, end_lr: 0., warmup_steps: 128}
  ngpu: 128
  lambda_rho: 0.

